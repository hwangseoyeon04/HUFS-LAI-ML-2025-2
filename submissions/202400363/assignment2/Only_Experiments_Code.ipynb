{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s4s7lwm7ea",
        "outputId": "c7c7339f-c0da-4274-a264-e8265943265c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "실험을 시작해보세요! 💪\n"
          ]
        }
      ],
      "source": [
        "# 실험을 위한 빈 셀\n",
        "# 여기서 위에서 제안한 실험들을 진행해보세요!\n",
        "\n",
        "# 예시: 학습률을 바꿔서 재훈련\n",
        "# learning_rate = 1e-2  # 원래보다 10배 큰 학습률\n",
        "# model = MLP().to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#\n",
        "# 위의 훈련 루프를 다시 실행하고 결과를 비교해보세요!\n",
        "\n",
        "print(\"실험을 시작해보세요! 💪\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지 설치\n",
        "!pip install datasets\n",
        "\n",
        "# 필요한 라이브러리 임포트 및 설정\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=100, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Sigmoid 활성화 함수를 사용하는 MLP 모델 정의\n",
        "class SigmoidMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=100, num_classes=10):\n",
        "        super(SigmoidMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Sigmoid(),  # ReLU 대신 Sigmoid 사용\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 은닉층 3개를 가진 DeepMLP 모델 정의\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size1=256, hidden_size2=128, hidden_size3=64, num_classes=10):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size3, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 데이터셋 로딩 및 전처리\n",
        "batch_size = 128\n",
        "test_batch_size = 1000\n",
        "nb_epochs = 3\n",
        "mnist = load_dataset(\"mnist\")\n",
        "sample_data = torch.stack([transforms.ToTensor()(mnist['train'][i]['image']) for i in range(1000)])\n",
        "mean = sample_data.mean().item()\n",
        "std = sample_data.std().item()\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((mean,), (std,))])\n",
        "def transform_dataset(dataset):\n",
        "    def transform_fn(batch):\n",
        "        images = [transform(img).view(-1) for img in batch[\"image\"]]\n",
        "        return {\"image\": torch.stack(images), \"label\": torch.tensor(batch[\"label\"])}\n",
        "    return dataset.with_transform(transform_fn)\n",
        "\n",
        "train_dataset = transform_dataset(mnist[\"train\"])\n",
        "test_dataset = transform_dataset(mnist[\"test\"])\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n공통 코드 준비 완료. 이제 아래의 실험 코드를 실행하세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MnvRFAOJdwa",
        "outputId": "201040f7-ead5-4733-ddf5-b08d38af858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "\n",
            "공통 코드 준비 완료. 이제 아래의 실험 코드를 실행하세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Learning Rate Tunning\n",
        "print(\"=== Experiment: Learning Rate Tunning ===\")\n",
        "learning_rates = [1e-2, 5e-3, 5e-4, 1e-4]\n",
        "nb_epochs = 3\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\n--- Learning Rate = {lr} ---\")\n",
        "    model = MLP().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        model.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            imgs = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                imgs = batch[\"image\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "        test_acc = 100 * correct_test / total_test\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{nb_epochs}] - Train Acc: {epoch_train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\nFinal Results for LR={lr}\")\n",
        "    print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zo5jUldJpXU",
        "outputId": "a86e6614-0565-4293-f709-67e7e2df82e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Learning Rate Tunning ===\n",
            "\n",
            "--- Learning Rate = 0.01 ---\n",
            "Epoch [1/3] - Train Acc: 92.11%, Test Acc: 94.75%\n",
            "Epoch [2/3] - Train Acc: 95.33%, Test Acc: 95.07%\n",
            "Epoch [3/3] - Train Acc: 95.75%, Test Acc: 95.51%\n",
            "\n",
            "Final Results for LR=0.01\n",
            "Final Train Accuracy: 95.75%\n",
            "Final Test Accuracy: 95.51%\n",
            "\n",
            "--- Learning Rate = 0.005 ---\n",
            "Epoch [1/3] - Train Acc: 92.85%, Test Acc: 95.19%\n",
            "Epoch [2/3] - Train Acc: 96.33%, Test Acc: 95.42%\n",
            "Epoch [3/3] - Train Acc: 97.12%, Test Acc: 96.92%\n",
            "\n",
            "Final Results for LR=0.005\n",
            "Final Train Accuracy: 97.12%\n",
            "Final Test Accuracy: 96.92%\n",
            "\n",
            "--- Learning Rate = 0.0005 ---\n",
            "Epoch [1/3] - Train Acc: 88.97%, Test Acc: 93.47%\n",
            "Epoch [2/3] - Train Acc: 94.43%, Test Acc: 95.54%\n",
            "Epoch [3/3] - Train Acc: 95.99%, Test Acc: 96.29%\n",
            "\n",
            "Final Results for LR=0.0005\n",
            "Final Train Accuracy: 95.99%\n",
            "Final Test Accuracy: 96.29%\n",
            "\n",
            "--- Learning Rate = 0.0001 ---\n",
            "Epoch [1/3] - Train Acc: 81.00%, Test Acc: 90.01%\n",
            "Epoch [2/3] - Train Acc: 90.55%, Test Acc: 91.59%\n",
            "Epoch [3/3] - Train Acc: 92.07%, Test Acc: 92.71%\n",
            "\n",
            "Final Results for LR=0.0001\n",
            "Final Train Accuracy: 92.07%\n",
            "Final Test Accuracy: 92.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Hidden Size Tunning\n",
        "print(\"=== Experiment: Hidden Size Tunning ===\")\n",
        "hidden_sizes = [50, 200, 500, 1000]\n",
        "learning_rate = 1e-3\n",
        "nb_epochs = 3\n",
        "\n",
        "for hs in hidden_sizes:\n",
        "    print(f\"\\n--- Hidden Size = {hs} ---\")\n",
        "    model = MLP(hidden_size=hs).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        model.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            imgs = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                imgs = batch[\"image\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "        test_acc = 100 * correct_test / total_test\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{nb_epochs}] - Train Acc: {epoch_train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\nFinal Results for Hidden Size={hs}\")\n",
        "    print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUHIcfLpJvSM",
        "outputId": "8d8cb22a-dcd6-4c01-8d7a-8410f37cb8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Hidden Size Tunning ===\n",
            "\n",
            "--- Hidden Size = 50 ---\n",
            "Epoch [1/3] - Train Acc: 89.51%, Test Acc: 93.71%\n",
            "Epoch [2/3] - Train Acc: 94.27%, Test Acc: 95.27%\n",
            "Epoch [3/3] - Train Acc: 95.62%, Test Acc: 96.00%\n",
            "\n",
            "Final Results for Hidden Size=50\n",
            "Final Train Accuracy: 95.62%\n",
            "Final Test Accuracy: 96.00%\n",
            "\n",
            "--- Hidden Size = 200 ---\n",
            "Epoch [1/3] - Train Acc: 91.78%, Test Acc: 95.68%\n",
            "Epoch [2/3] - Train Acc: 96.58%, Test Acc: 97.04%\n",
            "Epoch [3/3] - Train Acc: 97.61%, Test Acc: 96.95%\n",
            "\n",
            "Final Results for Hidden Size=200\n",
            "Final Train Accuracy: 97.61%\n",
            "Final Test Accuracy: 96.95%\n",
            "\n",
            "--- Hidden Size = 500 ---\n",
            "Epoch [1/3] - Train Acc: 93.02%, Test Acc: 96.24%\n",
            "Epoch [2/3] - Train Acc: 97.21%, Test Acc: 97.59%\n",
            "Epoch [3/3] - Train Acc: 98.19%, Test Acc: 97.61%\n",
            "\n",
            "Final Results for Hidden Size=500\n",
            "Final Train Accuracy: 98.19%\n",
            "Final Test Accuracy: 97.61%\n",
            "\n",
            "--- Hidden Size = 1000 ---\n",
            "Epoch [1/3] - Train Acc: 93.72%, Test Acc: 96.99%\n",
            "Epoch [2/3] - Train Acc: 97.42%, Test Acc: 97.02%\n",
            "Epoch [3/3] - Train Acc: 98.35%, Test Acc: 97.64%\n",
            "\n",
            "Final Results for Hidden Size=1000\n",
            "Final Train Accuracy: 98.35%\n",
            "Final Test Accuracy: 97.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Epochs Tunning\n",
        "print(\"=== Experiment: Epochs Tunning ===\")\n",
        "epochs_list = [5, 10, 20, 30]\n",
        "learning_rate = 1e-3\n",
        "hidden_size = 100\n",
        "\n",
        "for nb_epochs in epochs_list:\n",
        "    print(f\"\\n--- Number of Epochs = {nb_epochs} ---\")\n",
        "    model = MLP(hidden_size=hidden_size).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        model.train()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            imgs = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                imgs = batch[\"image\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "        test_acc = 100 * correct_test / total_test\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{nb_epochs}] - Train Acc: {epoch_train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\nFinal Results for Epochs={nb_epochs}\")\n",
        "    print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEaETF19JzxU",
        "outputId": "3a7678f5-86e1-40a3-8a8c-a8fbdd1d9830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Epochs Tunning ===\n",
            "\n",
            "--- Number of Epochs = 5 ---\n",
            "Epoch [1/5] - Train Acc: 90.84%, Test Acc: 94.77%\n",
            "Epoch [2/5] - Train Acc: 95.76%, Test Acc: 96.16%\n",
            "Epoch [3/5] - Train Acc: 96.91%, Test Acc: 96.73%\n",
            "Epoch [4/5] - Train Acc: 97.63%, Test Acc: 97.22%\n",
            "Epoch [5/5] - Train Acc: 98.06%, Test Acc: 97.16%\n",
            "\n",
            "Final Results for Epochs=5\n",
            "Final Train Accuracy: 98.06%\n",
            "Final Test Accuracy: 97.16%\n",
            "\n",
            "--- Number of Epochs = 10 ---\n",
            "Epoch [1/10] - Train Acc: 90.75%, Test Acc: 94.61%\n",
            "Epoch [2/10] - Train Acc: 95.67%, Test Acc: 95.91%\n",
            "Epoch [3/10] - Train Acc: 96.91%, Test Acc: 96.92%\n",
            "Epoch [4/10] - Train Acc: 97.68%, Test Acc: 96.89%\n",
            "Epoch [5/10] - Train Acc: 98.06%, Test Acc: 97.50%\n",
            "Epoch [6/10] - Train Acc: 98.34%, Test Acc: 97.81%\n",
            "Epoch [7/10] - Train Acc: 98.56%, Test Acc: 97.59%\n",
            "Epoch [8/10] - Train Acc: 98.90%, Test Acc: 97.56%\n",
            "Epoch [9/10] - Train Acc: 99.03%, Test Acc: 97.87%\n",
            "Epoch [10/10] - Train Acc: 99.13%, Test Acc: 97.79%\n",
            "\n",
            "Final Results for Epochs=10\n",
            "Final Train Accuracy: 99.13%\n",
            "Final Test Accuracy: 97.79%\n",
            "\n",
            "--- Number of Epochs = 20 ---\n",
            "Epoch [1/20] - Train Acc: 90.87%, Test Acc: 95.00%\n",
            "Epoch [2/20] - Train Acc: 95.63%, Test Acc: 96.37%\n",
            "Epoch [3/20] - Train Acc: 96.99%, Test Acc: 96.57%\n",
            "Epoch [4/20] - Train Acc: 97.62%, Test Acc: 97.35%\n",
            "Epoch [5/20] - Train Acc: 98.03%, Test Acc: 96.99%\n",
            "Epoch [6/20] - Train Acc: 98.32%, Test Acc: 97.51%\n",
            "Epoch [7/20] - Train Acc: 98.64%, Test Acc: 97.50%\n",
            "Epoch [8/20] - Train Acc: 98.88%, Test Acc: 97.60%\n",
            "Epoch [9/20] - Train Acc: 99.08%, Test Acc: 97.48%\n",
            "Epoch [10/20] - Train Acc: 99.19%, Test Acc: 97.82%\n",
            "Epoch [11/20] - Train Acc: 99.33%, Test Acc: 97.82%\n",
            "Epoch [12/20] - Train Acc: 99.40%, Test Acc: 97.65%\n",
            "Epoch [13/20] - Train Acc: 99.53%, Test Acc: 97.84%\n",
            "Epoch [14/20] - Train Acc: 99.54%, Test Acc: 97.78%\n",
            "Epoch [15/20] - Train Acc: 99.62%, Test Acc: 97.73%\n",
            "Epoch [16/20] - Train Acc: 99.68%, Test Acc: 97.36%\n",
            "Epoch [17/20] - Train Acc: 99.73%, Test Acc: 97.71%\n",
            "Epoch [18/20] - Train Acc: 99.70%, Test Acc: 97.77%\n",
            "Epoch [19/20] - Train Acc: 99.80%, Test Acc: 97.64%\n",
            "Epoch [20/20] - Train Acc: 99.63%, Test Acc: 97.19%\n",
            "\n",
            "Final Results for Epochs=20\n",
            "Final Train Accuracy: 99.63%\n",
            "Final Test Accuracy: 97.19%\n",
            "\n",
            "--- Number of Epochs = 30 ---\n",
            "Epoch [1/30] - Train Acc: 91.07%, Test Acc: 94.87%\n",
            "Epoch [2/30] - Train Acc: 95.83%, Test Acc: 96.39%\n",
            "Epoch [3/30] - Train Acc: 96.95%, Test Acc: 97.17%\n",
            "Epoch [4/30] - Train Acc: 97.63%, Test Acc: 97.22%\n",
            "Epoch [5/30] - Train Acc: 98.05%, Test Acc: 97.43%\n",
            "Epoch [6/30] - Train Acc: 98.33%, Test Acc: 97.25%\n",
            "Epoch [7/30] - Train Acc: 98.65%, Test Acc: 97.27%\n",
            "Epoch [8/30] - Train Acc: 98.85%, Test Acc: 97.67%\n",
            "Epoch [9/30] - Train Acc: 99.00%, Test Acc: 97.59%\n",
            "Epoch [10/30] - Train Acc: 99.18%, Test Acc: 97.60%\n",
            "Epoch [11/30] - Train Acc: 99.29%, Test Acc: 97.66%\n",
            "Epoch [12/30] - Train Acc: 99.39%, Test Acc: 97.71%\n",
            "Epoch [13/30] - Train Acc: 99.48%, Test Acc: 97.78%\n",
            "Epoch [14/30] - Train Acc: 99.46%, Test Acc: 97.79%\n",
            "Epoch [15/30] - Train Acc: 99.61%, Test Acc: 97.63%\n",
            "Epoch [16/30] - Train Acc: 99.65%, Test Acc: 97.92%\n",
            "Epoch [17/30] - Train Acc: 99.66%, Test Acc: 97.82%\n",
            "Epoch [18/30] - Train Acc: 99.71%, Test Acc: 97.82%\n",
            "Epoch [19/30] - Train Acc: 99.68%, Test Acc: 97.98%\n",
            "Epoch [20/30] - Train Acc: 99.74%, Test Acc: 98.03%\n",
            "Epoch [21/30] - Train Acc: 99.78%, Test Acc: 97.91%\n",
            "Epoch [22/30] - Train Acc: 99.74%, Test Acc: 97.90%\n",
            "Epoch [23/30] - Train Acc: 99.79%, Test Acc: 97.58%\n",
            "Epoch [24/30] - Train Acc: 99.68%, Test Acc: 97.78%\n",
            "Epoch [25/30] - Train Acc: 99.81%, Test Acc: 97.75%\n",
            "Epoch [26/30] - Train Acc: 99.85%, Test Acc: 97.77%\n",
            "Epoch [27/30] - Train Acc: 99.72%, Test Acc: 97.78%\n",
            "Epoch [28/30] - Train Acc: 99.70%, Test Acc: 97.64%\n",
            "Epoch [29/30] - Train Acc: 99.78%, Test Acc: 97.93%\n",
            "Epoch [30/30] - Train Acc: 99.90%, Test Acc: 97.65%\n",
            "\n",
            "Final Results for Epochs=30\n",
            "Final Train Accuracy: 99.90%\n",
            "Final Test Accuracy: 97.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4: Sigmoid Activation Function\n",
        "print(\"=== Experiment: Sigmoid Activation Function ===\")\n",
        "learning_rate = 1e-3\n",
        "nb_epochs = 3\n",
        "\n",
        "model_sigmoid = SigmoidMLP().to(device)\n",
        "optimizer_sigmoid = optim.Adam(model_sigmoid.parameters(), lr=learning_rate)\n",
        "\n",
        "train_accuracies_sigmoid = []\n",
        "test_accuracies_sigmoid = []\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    model_sigmoid.train()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        imgs = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        optimizer_sigmoid.zero_grad()\n",
        "        outputs = model_sigmoid(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_sigmoid.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_train_acc = 100 * correct_train / total_train\n",
        "    train_accuracies_sigmoid.append(epoch_train_acc)\n",
        "\n",
        "    model_sigmoid.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            imgs = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "            outputs = model_sigmoid(imgs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "    test_acc = 100 * correct_test / total_test\n",
        "    test_accuracies_sigmoid.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{nb_epochs}] - Train Acc: {epoch_train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nFinal Results for Sigmoid MLP\")\n",
        "print(f\"Final Train Accuracy: {train_accuracies_sigmoid[-1]:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_accuracies_sigmoid[-1]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbxFg-LpJ2rq",
        "outputId": "ec3b533b-d86a-4c62-99c0-315791fa4112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Sigmoid Activation Function ===\n",
            "Epoch [1/3] - Train Acc: 87.91%, Test Acc: 92.62%\n",
            "Epoch [2/3] - Train Acc: 93.58%, Test Acc: 94.45%\n",
            "Epoch [3/3] - Train Acc: 95.06%, Test Acc: 95.13%\n",
            "\n",
            "Final Results for Sigmoid MLP\n",
            "Final Train Accuracy: 95.06%\n",
            "Final Test Accuracy: 95.13%\n"
          ]
        }
      ]
    }
  ]
}