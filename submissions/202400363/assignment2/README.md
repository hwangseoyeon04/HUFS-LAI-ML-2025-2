
### MNIST 분류 실험 결과


## 기본 모델 성능

-최종 훈련 정확도: 97.00%
-최종 테스트 정확도: 97.03%
-과적합 정도: -0.03% (훈련-테스트 정확도 차이)

---

## 실험 결과

### 실험 1: 학습률(Learning Rate) 튜닝

-변경사항: 학습률을 `0.01`, `0.005`, `0.0005`, `0.0001`로 변경하며 모델을 훈련했습니다.
-결과:
    * LR = 0.01: 최종 테스트 정확도 95.51%
    * LR = 0.005: 최종 테스트 정확도 96.92%
    * LR = 0.0005: 최종 테스트 정확도 96.29%
    * LR = 0.0001: 최종 테스트 정확도 92.71%
-분석: 학습률이 너무 높거나(0.01) 너무 낮으면(0.0001) 모델의 성능이 저하되는 것을 확인했습니다. 학습률이 0.01일 때는 모델이 최적점에 도달하지 못하고 불안정하게 학습했을 가능성이 있고, 0.0001일 때는 학습 속도가 너무 느려 3번의 에포크로는 충분히 수렴하지 못했습니다. 최적의 성능은 `0.005` 부근에서 나타났습니다.

### 실험 2: 은닉층 크기(Hidden Size) 튜닝

-변경사항: 은닉층 크기를 `50`, `200`, `500`, `1000`으로 변경하며 모델을 훈련했습니다.
-결과:
    * Hidden Size = 50: 최종 테스트 정확도 96.00%
    * Hidden Size = 200: 최종 테스트 정확도 96.95%
    * Hidden Size = 500: 최종 테스트 정확도 97.61%
    * Hidden Size = 1000: 최종 테스트 정확도 97.64%
-분석: 은닉층의 크기가 증가할수록 모델의 표현력이 향상되어 테스트 정확도가 전반적으로 상승했습니다. 하지만 은닉층 크기를 500에서 1000으로 늘렸을 때 정확도 상승폭은 미미했습니다. 모델의 복잡도가 특정 수준을 넘어서면 성능 향상 효과가 크지 않은 것 같다고 분석하였습니다.

### 실험 3: 에포크 수(Epochs) 튜닝

-변경사항: 에포크 수를 `5`, `10`, `20`, `30`으로 증가시키며 모델을 훈련했습니다. (기본 모델: 3 에포크)
-결과:
    * Epochs = 5: 최종 테스트 정확도 97.16%
    * Epochs = 10: 최종 테스트 정확도 97.79%
    * Epochs = 20: 최종 테스트 정확도 97.19%
    * Epochs = 30: 최종 테스트 정확도 97.65%
-분석: 에포크 수를 늘릴수록 훈련 정확도는 꾸준히 99% 이상으로 상승했지만, 테스트 정확도는 에포크 10 이후부터 정체되거나 미세하게 하락하는 경향을 보였습니다. 이는 모델이 훈련 데이터에 너무 맞춰져 일반화 성능이 떨어져, 에포크 10 이후부터 과적합이 발생한 것이라고 예측하였습니다.

### 실험 4: 모델 구조 개선

-변경사항: 은닉층의 활성화 함수를 nn.ReLU()에서 nn.Sigmoid()로 변경했습니다.
-결과:
    * Sigmoid 활성화 함수: 최종 테스트 정확도 95.13%
-분석: ReLU 활성화 함수를 사용한 기본 모델(97.03%)에 비해 Sigmoid 활성화 함수를 사용했을 때 성능이 하락했습니다. 이는 Sigmoid 함수의 미분값이 0과 1 사이의 작은 값으로 제한되어, 역전파 과정에서 그래디언트가 소실되어 학습이 느려지거나 멈췄기 때문이라고 예측하였습니다. 
---

## 결론 및 인사이트

-가장 효과적인 개선 방법: 실험 결과들을 종합해보면, 은닉층의 크기를 늘리는 것이 가장 효과적인 성능 개선 방법이었습니다. 학습률의 경우 0.005가 가장 좋은 결과를 보여주었으며, 에포크 수는 10회에서 테스트 정확도가 최고치였습니다.
-관찰된 패턴:
    * 학습률은 너무 크거나 작지 않은 적절한 값을 찾아야 합니다.
    * 은닉층 크기가 클수록 초기에는 성능이 좋아지지만, 특정 크기 이후로는 성능 향상이 미미해집니다.
    * 에포크 수를 늘릴수록 훈련 정확도는 계속 상승하지만, 테스트 정확도는 에포크 10 이후 정체되거나 하락했습니다. 특정 시점 이후로는    과적합이 발생할 수 있습니다.
    * Sigmoid 함수는 기본 모델의 ReLU보다 성능이 낮게 나타났습니다.
-추가 개선 아이디어:
    * 과적합을 방지하기 위해 은닉층에 Dropout 레이어를 추가하는 실험을 해볼 수 있습니다.
    * nn.Sigmoid() 외에도 더 다양한 활성화 함수를 비교하여 성능을 분석해볼 수 있습니다.
    